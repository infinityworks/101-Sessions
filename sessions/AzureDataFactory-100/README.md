# Azure Data Factory 101 - Creating a data pipeline

The learning aims are to gain a basic understand and competency in creating data pipelines from scratch which can ingest `.csv` data from an Azure Blob Storage, and copy / move it to a Snowflake or Azure SQL Database.

## [Why Snowflake?](https://kb.infinityworks.com/snowflake/)
Check out the knowledge base article on why we use it.

## Who should take this workshop?

Anyone interested in building a data pipeline with Azure Data Factory which moves its data into an SQL or Snowflake database or those wishing to gain a basic understanding of Data Factory and Snowflake's web UIs.

## Pre-requisites

### Experience
- Ability to login to Snowflake.
- Ability to login to Azure.
- Working knowledge of SQL databases is preferred but not essential.

### Accounts
- Microsoft Azure account
- Snowflake account

### Tools
- Snowflake account with SYSADMIN role (create a free trial account and choose the enterprise edition).

- OS:
    - Ubuntu, Mac OS (UNIX based etc) strongly preferred
    - Windows not recommended??

Installations:
- Minimum (not recommended but acceptable):
    - ??
- Preferred:
    - ??


<!-- [See here for the Instructors guide](https://github.com/infinityworks/101-Sessions/blob/master/sessions/Snowflake-100/INSTRUCTORS_GUIDE.md) -->

## Sessions

This 101 course is split into two parts which could each take up to four hours to complete, depending on the knowledge level of the learners. 

### Learning Objectives
PART 1
- Identify the Key Features of Cloud Computing and Data Factory
- Define the Data Flow for a Basic Data Pipeline
- Create a Basic Pipeline with the Trainer

PART 2
- Create Another Basic Pipeline with Less Assistance
- Identify the Features of Data Factory Monitoring
- Monitor a Pipeline For Successes and Failures
- Resolve Common Pipeline Failures